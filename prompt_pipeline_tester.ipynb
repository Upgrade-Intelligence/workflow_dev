{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8567cf35",
   "metadata": {},
   "source": [
    "# Interactive Storybook Prompt Pipeline Tester\n",
    "\n",
    "Use this notebook to run and validate each prompt stage (1-5) and downstream image generation with Gemini 2.5 Flash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d7190",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "- Environment setup\n",
    "- Prompt loading utilities\n",
    "- Stage-by-stage testers (Stages 1-5)\n",
    "- Reference image prompt generation\n",
    "- Style-guided image generation\n",
    "- Full pipeline runner\n",
    "- Validation helpers\n",
    "\n",
    "> Stage 6 guardrail review is intentionally omitted for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddd618",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0186b",
   "metadata": {},
   "source": [
    "Configure library imports, load API credentials, and instantiate SDK clients.\n",
    "\n",
    "> The notebook falls back to `~/.zshrc` if keys are not present in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4f1e6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import textwrap\n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from IPython.display import JSON, Markdown, display\n",
    "\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "85486632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 OpenAI client ready (GPT-5 via Responses API)\n",
      "\u2705 Gemini client ready (Nano Banana / Gemini 2.5 Flash)\n",
      "\ud83d\udcc2 Outputs will be stored in: /Users/jacky/Tomo/Jacky/talkbookv1/docs/outputs/20251016-120035\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_ROOT = Path.cwd()\n",
    "PROMPT_DIR = NOTEBOOK_ROOT / 'prompts'\n",
    "STYLE_GUIDE_PATH = PROMPT_DIR / 'image_style_guidance.json'\n",
    "OUTPUT_ROOT = NOTEBOOK_ROOT / 'docs' / 'outputs'\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_api_key(env_var: str) -> Optional[str]:\n",
    "    key = os.getenv(env_var)\n",
    "    if key:\n",
    "        return key\n",
    "    zshrc = Path.home() / '.zshrc'\n",
    "    if zshrc.exists():\n",
    "        for line in zshrc.read_text().splitlines():\n",
    "            if line.startswith(f'{env_var}='):\n",
    "                value = line.split('=', 1)[1].strip()\n",
    "                if value and value[0] == value[-1] and value[0] in (\"\\\"\", \"'\"):\n",
    "                    value = value[1:-1]\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "OPENAI_API_KEY = load_api_key('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError('OPENAI_API_KEY is required. Set it in the environment or ~/.zshrc.')\n",
    "\n",
    "GEMINI_API_KEY = load_api_key('GEMINI_API_KEY')\n",
    "if not GEMINI_API_KEY:\n",
    "    raise EnvironmentError('GEMINI_API_KEY is required. Set it in the environment or ~/.zshrc.')\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY, max_retries=5)\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "TIMESTAMP = dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "RUN_OUTPUT_DIR = OUTPUT_ROOT / TIMESTAMP\n",
    "RUN_OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print('\u2705 OpenAI client ready (GPT-5 via Responses API)')\n",
    "print('\u2705 Gemini client ready (Nano Banana / Gemini 2.5 Flash)')\n",
    "print(f'\ud83d\udcc2 Outputs will be stored in: {RUN_OUTPUT_DIR}')\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb48ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prompt Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19c2d1",
   "metadata": {},
   "source": [
    "## Running & Saving Outputs\n",
    "\n",
    "Run the environment setup cell before any stage testers so the notebook knows where to write files.\n",
    "\n",
    "1. Execute the setup code that loads API keys and instantiates GPT-5/Gemini clients; it prints the active `RUN_OUTPUT_DIR` under `docs/outputs/`.\n",
    "2. Keep `DRY_RUN = False` when you want real responses\u2014dry runs skip the `write_stage_output` helper and nothing is saved.\n",
    "3. Re-run the setup cell any time you need a fresh timestamped folder before running stage cells or `run_full_pipeline`.\n",
    "4. After stages or the full pipeline finish, watch the console messages (or `print(RUN_OUTPUT_DIR)`) to confirm the saved files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34a139",
   "metadata": {},
   "source": [
    "Utility helpers for loading prompt templates, rendering them with runtime context, and storing raw model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "adfd9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_templates(prompt_dir: Path) -> Dict[str, str]:\n",
    "    templates: Dict[str, str] = {}\n",
    "    for path in sorted(prompt_dir.glob('*.txt')):\n",
    "        templates[path.stem] = path.read_text().strip()\n",
    "    return templates\n",
    "\n",
    "def render_prompt(template: str, context: Dict[str, Any]) -> str:\n",
    "    rendered = template\n",
    "    for key, value in context.items():\n",
    "        placeholder = '{{' + key + '}}'\n",
    "        rendered = rendered.replace(placeholder, str(value))\n",
    "    return rendered\n",
    "\n",
    "def write_stage_output(stage: str, content: str, suffix: str = 'txt') -> Path:\n",
    "    out_path = RUN_OUTPUT_DIR / f'{stage}.{suffix}'\n",
    "    out_path.write_text(content.strip())\n",
    "    return out_path\n",
    "\n",
    "def ensure_directory(path: Path) -> Path:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3197188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc4 Loaded 7 prompt templates from /Users/jacky/Tomo/Jacky/talkbookv1/prompts\n",
      "\ud83c\udfa8 Style guide suffix: Cheerful 2D digital illustration with soft pastel palette, gentle rounded linewo...\n",
      "\ud83d\udeab Negative prompt: No weapons, no frightening imagery, no mature themes, no harsh shadows, no cropp...\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATES = load_prompt_templates(PROMPT_DIR)\n",
    "STYLE_GUIDE = json.loads(STYLE_GUIDE_PATH.read_text())\n",
    "print(f'\ud83d\udcc4 Loaded {len(PROMPT_TEMPLATES)} prompt templates from {PROMPT_DIR}')\n",
    "print('\ud83c\udfa8 Style guide suffix:', STYLE_GUIDE.get('style_prompt_suffix', '')[:80] + '...')\n",
    "print('\ud83d\udeab Negative prompt:', STYLE_GUIDE.get('negative_prompt', '')[:80] + '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7007c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Stage Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c89e1",
   "metadata": {},
   "source": [
    "Helpers to invoke GPT-5 for stages 1-5, parse their structured outputs, and bridge into Gemini image requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a2c78395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "DRY_RUN = False  # Toggle to skip live API calls and rely on cached outputs\n",
    "DEFAULT_GPT5_MODEL = 'gpt-5'\n",
    "DEFAULT_REASONING_EFFORT = 'medium'\n",
    "REASONING_MODEL_PREFIXES = ('gpt-5', 'o')\n",
    "\n",
    "def call_gpt5(\n",
    "    prompt: str,\n",
    "    stage: str,\n",
    "    *,\n",
    "    model: str = DEFAULT_GPT5_MODEL,\n",
    "    reasoning_effort: Optional[str] = DEFAULT_REASONING_EFFORT,\n",
    "    instructions: Optional[str] = None,\n",
    "    max_output_tokens: int = 15000,\n",
    "    temperature: Optional[float] = None,\n",
    "    output_suffix: str = 'txt',\n",
    "    stream: bool = False,  # Default to False for more reliable operation\n",
    "    max_retries: int = 3,\n",
    ") -> str:\n",
    "    \"\"\"Invoke GPT-5 via Responses API and persist the raw output.\n",
    "\n",
    "    GPT-5 and other reasoning models ignore sampling controls such as `temperature`,\n",
    "    so we only forward that parameter for non-reasoning models per the OpenAI docs.\n",
    "    \"\"\"\n",
    "    if DRY_RUN:\n",
    "        cached_path = RUN_OUTPUT_DIR / f'{stage}.{output_suffix}'\n",
    "        if cached_path.exists():\n",
    "            print(f'[dry-run] Using cached response for {stage} -> {cached_path.name}')\n",
    "            return cached_path.read_text()\n",
    "        print(f'[dry-run] No cached response found for {stage}; returning empty string.')\n",
    "        return ''\n",
    "\n",
    "    input_payload = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'input_text', 'text': prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response_args = {\n",
    "        'model': model,\n",
    "        'input': input_payload,\n",
    "        'max_output_tokens': max_output_tokens,\n",
    "        'stream': stream,\n",
    "    }\n",
    "    if instructions:\n",
    "        response_args['instructions'] = instructions\n",
    "    if reasoning_effort:\n",
    "        response_args['reasoning'] = {'effort': reasoning_effort}\n",
    "    if temperature is not None:\n",
    "        if not model.startswith(REASONING_MODEL_PREFIXES):\n",
    "            response_args['temperature'] = temperature\n",
    "        else:\n",
    "            print(f\"[info] temperature parameter ignored for reasoning model {model} per GPT-5 API guidance.\")\n",
    "\n",
    "    # Retry logic for connection issues\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if stream:\n",
    "                print(f'\ud83d\udd04 Streaming response for {stage}... (attempt {attempt + 1})')\n",
    "                full_text = ''\n",
    "                \n",
    "                for chunk in openai_client.responses.create(**response_args):\n",
    "                    if hasattr(chunk, 'output_text') and chunk.output_text:\n",
    "                        print(chunk.output_text, end='', flush=True)\n",
    "                        full_text += chunk.output_text\n",
    "                \n",
    "                print()  # New line after streaming completes\n",
    "                text = full_text\n",
    "            else:\n",
    "                print(f'\ud83d\udd04 Calling GPT-5 for {stage}... (attempt {attempt + 1})')\n",
    "                response = openai_client.responses.create(**response_args)\n",
    "                text = response.output_text or ''\n",
    "            \n",
    "            # If we get here, the request succeeded\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'\u274c Attempt {attempt + 1} failed: {e}')\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n",
    "                print(f'\u23f3 Retrying in {wait_time} seconds...')\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f'\ud83d\udca5 All {max_retries} attempts failed for {stage}')\n",
    "                raise\n",
    "    \n",
    "    write_stage_output(stage, text, suffix=output_suffix)\n",
    "    return text\n",
    "\n",
    "def parse_json_response(raw_text: str, stage: str) -> Dict[str, Any]:\n",
    "    raw = raw_text.strip()\n",
    "    if not raw:\n",
    "        raise ValueError(f'No JSON content returned for {stage}.')\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError as err:\n",
    "        raise ValueError(f'Failed to parse JSON for {stage}: {err}') from err\n",
    "\n",
    "def parse_xml_response(raw_text: str, stage: str) -> ET.Element:\n",
    "    raw = raw_text.strip()\n",
    "    if not raw:\n",
    "        raise ValueError(f'No XML content returned for {stage}.')\n",
    "    try:\n",
    "        return ET.fromstring(raw)\n",
    "    except ET.ParseError as err:\n",
    "        raise ValueError(f'Failed to parse XML for {stage}: {err}') from err\n",
    "\n",
    "def stage_result(stage: str, prompt: str, raw: str, parsed: Any) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'stage': stage,\n",
    "        'prompt': prompt,\n",
    "        'raw': raw,\n",
    "        'parsed': parsed,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "13443efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_nodes_xml_to_records(root: ET.Element) -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    for node in root.findall('Node'):\n",
    "        record: Dict[str, Any] = {\n",
    "            'node_id': node.findtext('Id'),\n",
    "            'type': node.findtext('Type') or 'linear',\n",
    "            'next_node': node.findtext('NextNode'),\n",
    "            'parent_choice_id': node.findtext('ParentChoiceId'),\n",
    "            'continuity_notes': node.findtext('ContinuityNotes'),\n",
    "            'content': node.findtext('NodeContent') or node.findtext('SceneText'),\n",
    "            'retry_message': node.findtext('RetryMessage'),\n",
    "        }\n",
    "        choices_elem = node.find('Choices')\n",
    "        if choices_elem is not None:\n",
    "            record['choices'] = []\n",
    "            for choice in choices_elem.findall('Choice'):\n",
    "                record['choices'].append({\n",
    "                    'id': choice.findtext('Id'),\n",
    "                    'text': choice.findtext('ChoiceText'),\n",
    "                    'is_correct': (choice.findtext('IsCorrect') or '').lower() == 'true',\n",
    "                    'next_node': choice.findtext('NextNode'),\n",
    "                })\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "def storyboard_nodes_xml_to_records(root: ET.Element) -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    for node in root.findall('Node'):\n",
    "        scene_text = (node.findtext('SceneText') or '').strip()\n",
    "        image_prompt = (node.findtext('ImagePrompt') or '').strip()\n",
    "        character_ids: List[str] = []\n",
    "        char_list_elem = node.find('CharacterList')\n",
    "        if char_list_elem is not None:\n",
    "            for character_id_elem in char_list_elem.findall('CharacterId'):\n",
    "                character_id = (character_id_elem.text or '').strip()\n",
    "                if character_id and character_id not in character_ids:\n",
    "                    character_ids.append(character_id)\n",
    "        record: Dict[str, Any] = {\n",
    "            'node_id': node.findtext('Id'),\n",
    "            'type': node.findtext('Type') or 'linear',\n",
    "            'next_node': node.findtext('NextNode'),\n",
    "            'parent_choice_id': node.findtext('ParentChoiceId'),\n",
    "            'continuity_notes': node.findtext('ContinuityNotes'),\n",
    "            'scene_text': scene_text,\n",
    "            'image_prompt': image_prompt,\n",
    "            'display_text': scene_text,\n",
    "            'retry_message': (node.findtext('RetryMessage') or '').strip() or None,\n",
    "            'character_ids': character_ids,\n",
    "        }\n",
    "        characters_data = [{'character_id': cid} for cid in character_ids]\n",
    "        illustration_brief: Dict[str, Any] = {\n",
    "            'prompt': image_prompt,\n",
    "            'characters': characters_data,\n",
    "        }\n",
    "        record['illustration_brief'] = illustration_brief\n",
    "        choices_elem = node.find('Choices')\n",
    "        if choices_elem is not None:\n",
    "            record['choices'] = []\n",
    "            for choice in choices_elem.findall('Choice'):\n",
    "                choice_entry: Dict[str, Any] = {\n",
    "                    'id': choice.findtext('Id'),\n",
    "                    'next_node': choice.findtext('NextNode'),\n",
    "                    'text': choice.findtext('ChoiceText') or '',\n",
    "                    'is_correct': (choice.findtext('IsCorrect') or '').lower() == 'true',\n",
    "                }\n",
    "                record['choices'].append(choice_entry)\n",
    "        extra_fields: Dict[str, Any] = {}\n",
    "        skip_tags = {'Id', 'Type', 'NextNode', 'ParentChoiceId', 'ContinuityNotes', 'SceneText', 'ImagePrompt', 'RetryMessage', 'Choices', 'CharacterList'}\n",
    "        for child in list(node):\n",
    "            if child.tag in skip_tags:\n",
    "                continue\n",
    "            extra_fields[child.tag] = (child.text or '').strip()\n",
    "        if extra_fields:\n",
    "            record['extra_fields'] = extra_fields\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "def extract_choice_node_set(root: ET.Element) -> Dict[str, Any]:\n",
    "    question_nodes: List[Dict[str, Any]] = []\n",
    "    outcome_nodes: List[Dict[str, Any]] = []\n",
    "    for node in root.findall('Node'):\n",
    "        node_type = (node.findtext('Type') or 'linear').lower()\n",
    "        node_id = node.findtext('Id')\n",
    "        next_node = node.findtext('NextNode')\n",
    "        if node_type == 'choice_question':\n",
    "            question_nodes.append({\n",
    "                'node_id': node_id,\n",
    "                'question_text': node.findtext('NodeContent') or node.findtext('SceneText'),\n",
    "                'retry_message': node.findtext('RetryMessage') or '',\n",
    "                'choices': [\n",
    "                    {\n",
    "                        'id': choice.findtext('Id'),\n",
    "                        'text': choice.findtext('ChoiceText'),\n",
    "                        'is_correct': (choice.findtext('IsCorrect') or '').lower() == 'true',\n",
    "                        'next_node': choice.findtext('NextNode'),\n",
    "                    }\n",
    "                    for choice in (node.find('Choices') or ET.Element('Choices')).findall('Choice')\n",
    "                ],\n",
    "            })\n",
    "        elif node_type == 'choice_outcome':\n",
    "            outcome_nodes.append({\n",
    "                'node_id': node_id,\n",
    "                'parent_choice_id': node.findtext('ParentChoiceId'),\n",
    "                'outcome_text': node.findtext('NodeContent') or node.findtext('SceneText'),\n",
    "                'next_node': next_node,\n",
    "            })\n",
    "    return {\n",
    "        'question_nodes': question_nodes,\n",
    "        'outcome_nodes': outcome_nodes,\n",
    "    }\n",
    "\n",
    "def to_pretty_json(data: Any) -> str:\n",
    "    return json.dumps(data, separators=(',', ':'), ensure_ascii=False)\n",
    "\n",
    "def story_records_to_graph(records: List[Dict[str, Any]], story_id: Optional[str] = None) -> Dict[str, Any]:\n",
    "    payload = {'nodes': records}\n",
    "    if story_id:\n",
    "        payload['story_id'] = story_id\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage1_learning_framework(user_inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    template = PROMPT_TEMPLATES['stage1_educational_theme_integrator']\n",
    "    context = {\n",
    "        'child_age': user_inputs.get('child_age', ''),\n",
    "        'theme': user_inputs.get('theme', ''),\n",
    "        'parental_guidance': user_inputs.get('parental_guidance', ''),\n",
    "        'character_preferences': user_inputs.get('character_preferences', ''),\n",
    "        'plot_preferences': user_inputs.get('plot_preferences', ''),\n",
    "    }\n",
    "    prompt_text = render_prompt(template, context)\n",
    "    raw_text = call_gpt5(prompt_text, 'stage1_learning_framework', output_suffix='json')\n",
    "    parsed = parse_json_response(raw_text, 'stage1_learning_framework') if raw_text else {}\n",
    "    return stage_result('stage1_learning_framework', prompt_text, raw_text, parsed)\n",
    "\n",
    "def run_stage2_story(user_inputs: Dict[str, Any], learning_framework: Any) -> Dict[str, Any]:\n",
    "    template = PROMPT_TEMPLATES['stage2_story_generator']\n",
    "    if isinstance(learning_framework, dict):\n",
    "        framework_json = to_pretty_json(learning_framework)\n",
    "    elif isinstance(learning_framework, str):\n",
    "        framework_json = learning_framework\n",
    "    else:\n",
    "        raise ValueError('learning_framework must be dict or JSON string')\n",
    "    context = {\n",
    "        'learning_framework_json': framework_json,\n",
    "        'child_age': user_inputs.get('child_age', ''),\n",
    "        'theme': user_inputs.get('theme', ''),\n",
    "        'parental_guidance': user_inputs.get('parental_guidance', ''),\n",
    "        'character_preferences': user_inputs.get('character_preferences', ''),\n",
    "        'plot_preferences': user_inputs.get('plot_preferences', ''),\n",
    "    }\n",
    "    prompt_text = render_prompt(template, context)\n",
    "    raw_text = call_gpt5(prompt_text, 'stage2_story_generator', output_suffix='json')\n",
    "    parsed = parse_json_response(raw_text, 'stage2_story_generator') if raw_text else {}\n",
    "    return stage_result('stage2_story_generator', prompt_text, raw_text, parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "62ac1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage3_nodes(story_draft: Any, node_count: int = 10) -> Dict[str, Any]:\n",
    "    template = PROMPT_TEMPLATES['stage3_story_orchestrator']\n",
    "    if isinstance(story_draft, dict):\n",
    "        story_json = to_pretty_json(story_draft)\n",
    "    elif isinstance(story_draft, str):\n",
    "        story_json = story_draft\n",
    "    else:\n",
    "        raise ValueError('story_draft must be dict or JSON string')\n",
    "    context = {\n",
    "        'story_draft_json': story_json,\n",
    "        'node_count': node_count,\n",
    "    }\n",
    "    prompt_text = render_prompt(template, context)\n",
    "    raw_text = call_gpt5(prompt_text, 'stage3_story_orchestrator', output_suffix='xml')\n",
    "    parsed_records: List[Dict[str, Any]] = []\n",
    "    if raw_text:\n",
    "        root = parse_xml_response(raw_text, 'stage3_story_orchestrator')\n",
    "        parsed_records = story_nodes_xml_to_records(root)\n",
    "    return stage_result('stage3_story_orchestrator', prompt_text, raw_text, {'nodes': parsed_records})\n",
    "\n",
    "def run_stage4_choices(story_node_graph_xml: str, learning_framework: Any, desired_choice_count: int = 4) -> Dict[str, Any]:\n",
    "    template = PROMPT_TEMPLATES['stage4_choice_node_generator']\n",
    "    if isinstance(learning_framework, dict):\n",
    "        framework_json = to_pretty_json(learning_framework)\n",
    "    elif isinstance(learning_framework, str):\n",
    "        framework_json = learning_framework\n",
    "    else:\n",
    "        raise ValueError('learning_framework must be dict or JSON string')\n",
    "    context = {\n",
    "        'story_node_graph_xml': story_node_graph_xml,\n",
    "        'learning_framework_json': framework_json,\n",
    "        'desired_choice_count': desired_choice_count,\n",
    "    }\n",
    "    prompt_text = render_prompt(template, context)\n",
    "    raw_text = call_gpt5(prompt_text, 'stage4_choice_node_generator', output_suffix='xml')\n",
    "    parsed: Dict[str, Any] = {'nodes': [], 'choice_node_set': {}}\n",
    "    if raw_text:\n",
    "        root = parse_xml_response(raw_text, 'stage4_choice_node_generator')\n",
    "        parsed['nodes'] = story_nodes_xml_to_records(root)\n",
    "        parsed['choice_node_set'] = extract_choice_node_set(root)\n",
    "    return stage_result('stage4_choice_node_generator', prompt_text, raw_text, parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "84c1b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_MODEL = 'gemini-2.5-flash-image'\n",
    "\n",
    "\n",
    "def run_reference_image_prompts(character_bible: Any, visual_style_defaults: str = '', style_guidance: str = '') -> Dict[str, Any]:\n",
    "    template = PROMPT_TEMPLATES['reference_image_generator']\n",
    "    if isinstance(character_bible, dict):\n",
    "        bible_json = to_pretty_json(character_bible)\n",
    "    elif isinstance(character_bible, list):\n",
    "        bible_json = to_pretty_json(character_bible)\n",
    "    elif isinstance(character_bible, str):\n",
    "        bible_json = character_bible\n",
    "    else:\n",
    "        raise ValueError('character_bible must be dict, list, or JSON string')\n",
    "    context = {\n",
    "        'character_bible_json': bible_json,\n",
    "        'visual_style_defaults': visual_style_defaults,\n",
    "        'style_guidance': style_guidance,\n",
    "    }\n",
    "    prompt_text = render_prompt(template, context)\n",
    "    raw_text = call_gpt5(prompt_text, 'reference_image_generator', output_suffix='json')\n",
    "    parsed = parse_json_response(raw_text, 'reference_image_generator') if raw_text else {}\n",
    "    return stage_result('reference_image_generator', prompt_text, raw_text, parsed)\n",
    "\n",
    "\n",
    "def run_stage5_storyboard(\n",
    "    story_node_graph: Any,\n",
    "    choice_node_set: Any,\n",
    "    character_bible: Any,\n",
    "    reference_images: Any,\n",
    "    style_defaults: Any = None,\n",
    "    *,\n",
    "    story_node_graph_xml: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    template = PROMPT_TEMPLATES['stage5_storyboarder']\n",
    "\n",
    "    def to_json_payload(obj: Any) -> str:\n",
    "        if isinstance(obj, str):\n",
    "            return obj\n",
    "        return to_pretty_json(obj)\n",
    "\n",
    "    xml_payload = story_node_graph_xml\n",
    "    if not xml_payload and isinstance(story_node_graph, str) and story_node_graph.strip().startswith('<'):\n",
    "        xml_payload = story_node_graph\n",
    "\n",
    "    context = {\n",
    "        'story_node_graph_xml': xml_payload or '',\n",
    "        'story_node_graph_json': to_json_payload(story_node_graph),\n",
    "        'choice_node_set_json': to_json_payload(choice_node_set),\n",
    "        'character_bible_json': to_json_payload(character_bible),\n",
    "        'reference_images_json': to_json_payload(reference_images),\n",
    "        'style_defaults': to_json_payload(style_defaults or STYLE_GUIDE),\n",
    "    }\n",
    "    prompt_text = render_prompt(template, context)\n",
    "    raw_text = call_gpt5(prompt_text, 'stage5_storyboarder', output_suffix='xml')\n",
    "    parsed: Dict[str, Any] = {}\n",
    "    if raw_text:\n",
    "        root = parse_xml_response(raw_text, 'stage5_storyboarder')\n",
    "        nodes = storyboard_nodes_xml_to_records(root)\n",
    "        parsed = {\n",
    "            'storyboard_nodes': {\n",
    "                'nodes': nodes,\n",
    "                'raw_xml': raw_text,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return stage_result('stage5_storyboarder', prompt_text, raw_text, parsed)\n",
    "\n",
    "def save_inline_content_images(response: Any, output_dir: Path, stem: str) -> List[Path]:\n",
    "    ensure_directory(output_dir)\n",
    "    saved_paths: List[Path] = []\n",
    "    if not response or not getattr(response, 'candidates', None):\n",
    "        return saved_paths\n",
    "    for candidate_index, candidate in enumerate(response.candidates, start=1):\n",
    "        content = getattr(candidate, 'content', None)\n",
    "        if not content:\n",
    "            continue\n",
    "        for part_index, part in enumerate(content.parts, start=1):\n",
    "            inline = getattr(part, 'inline_data', None)\n",
    "            if inline and getattr(inline, 'data', None):\n",
    "                data = inline.data\n",
    "                if isinstance(data, str):\n",
    "                    image_bytes = base64.b64decode(data)\n",
    "                else:\n",
    "                    image_bytes = data\n",
    "                image = Image.open(BytesIO(image_bytes))\n",
    "                mime_type = getattr(inline, 'mime_type', None) or 'image/png'\n",
    "                extension = 'png' if 'png' in mime_type else 'jpg'\n",
    "                base_path = output_dir / f\"{stem}.{extension}\"\n",
    "                file_path = base_path\n",
    "                if file_path.exists():\n",
    "                    counter = 2\n",
    "                    while True:\n",
    "                        candidate_path = output_dir / f\"{stem}_{counter}.{extension}\"\n",
    "                        if not candidate_path.exists():\n",
    "                            file_path = candidate_path\n",
    "                            break\n",
    "                        counter += 1\n",
    "                image.save(file_path)\n",
    "                saved_paths.append(file_path)\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def generate_reference_images(reference_prompts: Dict[str, Any], *, model: str = DEFAULT_IMAGE_MODEL, style_suffix: Optional[str] = None, negative_prompt: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    entries = (reference_prompts or {}).get('reference_prompts') or []\n",
    "    if limit is not None:\n",
    "        entries = entries[:limit]\n",
    "    assets: List[Dict[str, Any]] = []\n",
    "    output_dir = ensure_directory(RUN_OUTPUT_DIR / 'reference_images')\n",
    "    for index, entry in enumerate(entries, start=1):\n",
    "        prompt_text = entry.get('prompt_text') or entry.get('prompt') or entry.get('description')\n",
    "        if not prompt_text:\n",
    "            continue\n",
    "        pieces = [prompt_text]\n",
    "        if style_suffix:\n",
    "            pieces.append(f\"Style: {style_suffix}\")\n",
    "        if negative_prompt:\n",
    "            pieces.append(f\"Avoid: {negative_prompt}\")\n",
    "        composed_prompt = '\\n'.join(pieces)\n",
    "        if DRY_RUN:\n",
    "            print(f\"[dry-run] Skipping reference image generation for {entry.get('character_id')}\")\n",
    "            assets.append({'character_id': entry.get('character_id'), 'prompt': composed_prompt, 'image_paths': []})\n",
    "            continue\n",
    "        response = gemini_client.models.generate_content(model=model, contents=[composed_prompt])\n",
    "        image_paths = save_inline_content_images(response, output_dir, entry.get('character_id') or f'reference_{index}')\n",
    "        assets.append({\n",
    "            'character_id': entry.get('character_id'),\n",
    "            'prompt': composed_prompt,\n",
    "            'image_paths': [str(p) for p in image_paths],\n",
    "        })\n",
    "    return assets\n",
    "\n",
    "\n",
    "def build_reference_lookup(reference_assets: List[Dict[str, Any]]) -> Dict[str, str]:\n",
    "    lookup: Dict[str, str] = {}\n",
    "    for asset in reference_assets:\n",
    "        paths = asset.get('image_paths') or []\n",
    "        if asset.get('character_id') and paths:\n",
    "            lookup[asset['character_id']] = paths[0]\n",
    "    return lookup\n",
    "\n",
    "\n",
    "def compose_story_prompt(node: Dict[str, Any], style_guide: Dict[str, Any]) -> str:\n",
    "    parts: List[str] = []\n",
    "    display_text = node.get('display_text') or node.get('scene_text')\n",
    "    if display_text:\n",
    "        parts.append(display_text)\n",
    "    illustration = node.get('illustration_brief', {}) or {}\n",
    "    setting = illustration.get('setting')\n",
    "    if setting:\n",
    "        parts.append(f\"Setting details: {setting}\")\n",
    "    mood = illustration.get('mood_palette')\n",
    "    if mood:\n",
    "        parts.append(f\"Mood palette: {mood}\")\n",
    "    safety = illustration.get('safety_notes')\n",
    "    if safety:\n",
    "        parts.append(f\"Safety notes: {safety}\")\n",
    "    image_prompt = node.get('image_prompt') or illustration.get('prompt')\n",
    "    if image_prompt:\n",
    "        parts.append(f\"Illustration focus: {image_prompt}\")\n",
    "    style_suffix = style_guide.get('style_prompt_suffix')\n",
    "    if style_suffix:\n",
    "        parts.append(f\"Style: {style_suffix}\")\n",
    "    negative_prompt = style_guide.get('negative_prompt')\n",
    "    if negative_prompt:\n",
    "        parts.append(f\"Please avoid: {negative_prompt}\")\n",
    "    return '\\n'.join(parts)\n",
    "\n",
    "def generate_story_images_with_references(storyboard: Dict[str, Any], reference_assets: List[Dict[str, Any]], *, model: str = DEFAULT_IMAGE_MODEL, style_guide: Optional[Dict[str, Any]] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    style = style_guide or STYLE_GUIDE\n",
    "    nodes = storyboard.get('nodes', [])\n",
    "    if limit is not None:\n",
    "        nodes = nodes[:limit]\n",
    "    output_dir = ensure_directory(RUN_OUTPUT_DIR / 'images')\n",
    "    reference_lookup = build_reference_lookup(reference_assets)\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for node in nodes:\n",
    "        node_id = node.get('node_id')\n",
    "        illustration = node.get('illustration_brief', {})\n",
    "        characters = illustration.get('characters', [])\n",
    "        prompt_text = compose_story_prompt(node, style)\n",
    "        used_references: List[Dict[str, Any]] = []\n",
    "        if DRY_RUN:\n",
    "            print(f\"[dry-run] Skipping story image generation for node {node_id}\")\n",
    "            results.append({\n",
    "                'node_id': node_id,\n",
    "                'prompt': prompt_text,\n",
    "                'image_paths': [],\n",
    "                'references_used': used_references,\n",
    "            })\n",
    "            continue\n",
    "        parts: List[types.Part] = [types.Part.from_text(text=prompt_text)]\n",
    "        for char in characters:\n",
    "            char_id = char.get('character_id')\n",
    "            ref_path = reference_lookup.get(char_id)\n",
    "            if not ref_path:\n",
    "                continue\n",
    "            try:\n",
    "                data = Path(ref_path).read_bytes()\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            import mimetypes\n",
    "            mime_type = mimetypes.guess_type(ref_path)[0] or 'image/png'\n",
    "            parts.append(types.Part.from_bytes(data=data, mime_type=mime_type))\n",
    "            used_references.append({'character_id': char_id, 'image_path': ref_path})\n",
    "        contents = [types.Content(role='user', parts=parts)]\n",
    "        response = gemini_client.models.generate_content(model=model, contents=contents)\n",
    "        image_paths = save_inline_content_images(response, output_dir, node_id or 'node')\n",
    "        results.append({\n",
    "            'node_id': node_id,\n",
    "            'prompt': prompt_text,\n",
    "            'image_paths': [str(p) for p in image_paths],\n",
    "            'references_used': used_references,\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db8824eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_metadata_in_story_xml(\n",
    "    story_metadata: Any,\n",
    "    storyboard_xml: str,\n",
    "    *,\n",
    "    language: str = 'en-US',\n",
    "    version: str = '2.0',\n",
    "    read_time_minutes: int = 6,\n",
    "    target_age_range: str = '3-5',\n",
    ") -> str:\n",
    "    if not storyboard_xml:\n",
    "        return ''\n",
    "    try:\n",
    "        storyboard_root = ET.fromstring(storyboard_xml)\n",
    "    except ET.ParseError as exc:\n",
    "        raise ValueError('Invalid storyboard XML') from exc\n",
    "\n",
    "    # Normalize NextNode values so the final node (or any \"END\") points to null.\n",
    "    for node in storyboard_root.findall('Node'):\n",
    "        next_elem = node.find('NextNode')\n",
    "        if next_elem is None:\n",
    "            next_elem = ET.SubElement(node, 'NextNode')\n",
    "        if next_elem.text is None or next_elem.text.strip().upper() == 'END':\n",
    "            next_elem.text = 'null'\n",
    "\n",
    "    metadata_dict = story_metadata if isinstance(story_metadata, dict) else {}\n",
    "    story_id = metadata_dict.get('story_id') or 'unknown-story'\n",
    "    title = metadata_dict.get('title') or ''\n",
    "    theme = metadata_dict.get('theme') or ''\n",
    "    moral = metadata_dict.get('moral') or ''\n",
    "\n",
    "    root = ET.Element(\n",
    "        'InteractiveStorybook',\n",
    "        {\n",
    "            'story_id': story_id,\n",
    "            'version': version,\n",
    "            'language': language,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    metadata_elem = ET.SubElement(root, 'Metadata')\n",
    "    ET.SubElement(metadata_elem, 'Title').text = title\n",
    "    ET.SubElement(metadata_elem, 'Theme').text = theme\n",
    "    ET.SubElement(metadata_elem, 'Moral').text = moral\n",
    "    ET.SubElement(metadata_elem, 'ReadTime', {'minutes': str(read_time_minutes)})\n",
    "    ET.SubElement(metadata_elem, 'TargetAgeRange').text = target_age_range\n",
    "\n",
    "    root.append(storyboard_root)\n",
    "\n",
    "    return ET.tostring(root, encoding='utf-8', xml_declaration=True).decode('utf-8')\n",
    "\n",
    "\n",
    "def save_story_with_metadata(\n",
    "    story_metadata: Any,\n",
    "    storyboard_xml: str,\n",
    "    *,\n",
    "    output_dir: Path = RUN_OUTPUT_DIR,\n",
    "    filename: str = 'storyboard_with_metadata.xml',\n",
    ") -> Tuple[str, Optional[Path]]:\n",
    "    combined_xml = embed_metadata_in_story_xml(story_metadata, storyboard_xml)\n",
    "    if not combined_xml:\n",
    "        return '', None\n",
    "    output_path = output_dir / filename\n",
    "    output_path.write_text(combined_xml)\n",
    "    return combined_xml, output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "48c0045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_TTS_MODEL = 'gemini-2.5-flash-preview-tts'\n",
    "DEFAULT_TTS_VOICE = 'Kore'\n",
    "TTS_SAMPLE_RATE = 24000\n",
    "TTS_SAMPLE_WIDTH = 2\n",
    "TTS_CHANNELS = 1\n",
    "\n",
    "\n",
    "def write_pcm_to_wav(pcm_bytes: bytes, path: Path, *, sample_rate: int = TTS_SAMPLE_RATE, channels: int = TTS_CHANNELS, sample_width: int = TTS_SAMPLE_WIDTH) -> Path:\n",
    "    ensure_directory(path.parent)\n",
    "    with wave.open(str(path), 'wb') as wav_file:\n",
    "        wav_file.setnchannels(channels)\n",
    "        wav_file.setsampwidth(sample_width)\n",
    "        wav_file.setframerate(sample_rate)\n",
    "        wav_file.writeframes(pcm_bytes)\n",
    "    return path\n",
    "\n",
    "\n",
    "def extract_inline_audio_bytes(response: Any) -> bytes:\n",
    "    if not getattr(response, 'candidates', None):\n",
    "        return b''\n",
    "    for candidate in response.candidates:\n",
    "        content = getattr(candidate, 'content', None)\n",
    "        if not content:\n",
    "            continue\n",
    "        for part in getattr(content, 'parts', []) or []:\n",
    "            inline = getattr(part, 'inline_data', None)\n",
    "            if inline and getattr(inline, 'data', None):\n",
    "                data = inline.data\n",
    "                if isinstance(data, str):\n",
    "                    return base64.b64decode(data)\n",
    "                return data\n",
    "    return b''\n",
    "\n",
    "\n",
    "def call_gemini_tts(text: str, *, model: str = DEFAULT_TTS_MODEL, voice_name: str = DEFAULT_TTS_VOICE) -> bytes:\n",
    "    generation_config = types.GenerateContentConfig(\n",
    "        response_modalities=['AUDIO'],\n",
    "        speech_config=types.SpeechConfig(\n",
    "            voice_config=types.VoiceConfig(\n",
    "                prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=voice_name)\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=[types.Part.from_text(text=text)],\n",
    "        config=generation_config,\n",
    "    )\n",
    "    return extract_inline_audio_bytes(response)\n",
    "\n",
    "\n",
    "def generate_storyboard_tts(\n",
    "    storyboard: Dict[str, Any],\n",
    "    *,\n",
    "    model: str = DEFAULT_TTS_MODEL,\n",
    "    voice_name: str = DEFAULT_TTS_VOICE,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    limit: Optional[int] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    nodes = (storyboard or {}).get('nodes') if isinstance(storyboard, dict) else storyboard\n",
    "    if not nodes:\n",
    "        return []\n",
    "    target_dir = ensure_directory(output_dir or (RUN_OUTPUT_DIR / 'audio'))\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for index, node in enumerate(nodes):\n",
    "        if limit is not None and index >= limit:\n",
    "            break\n",
    "        node_id = node.get('node_id')\n",
    "        if not node_id:\n",
    "            continue\n",
    "        scene_text = (node.get('scene_text') or node.get('display_text') or '').strip()\n",
    "        retry_text = (node.get('retry_message') or '').strip()\n",
    "        choices = node.get('choices') or []\n",
    "        scene_audio_path = None\n",
    "        retry_audio_path = None\n",
    "        choice_audio_entries: List[Dict[str, Optional[str]]] = []\n",
    "\n",
    "        if scene_text:\n",
    "            filename = f\"{node_id}.wav\"\n",
    "            if DRY_RUN:\n",
    "                print(f\"[dry-run] Skipping TTS for node {node_id}\")\n",
    "            else:\n",
    "                try:\n",
    "                    audio_bytes = call_gemini_tts(scene_text, model=model, voice_name=voice_name)\n",
    "                    if audio_bytes:\n",
    "                        scene_audio_path = write_pcm_to_wav(audio_bytes, target_dir / filename)\n",
    "                except Exception as exc:\n",
    "                    print(f\"[warning] TTS failed for node {node_id}: {exc}\")\n",
    "\n",
    "        if retry_text:\n",
    "            retry_filename = f\"{node_id}_retry.wav\"\n",
    "            if DRY_RUN:\n",
    "                print(f\"[dry-run] Skipping retry TTS for node {node_id}\")\n",
    "            else:\n",
    "                try:\n",
    "                    retry_bytes = call_gemini_tts(retry_text, model=model, voice_name=voice_name)\n",
    "                    if retry_bytes:\n",
    "                        retry_audio_path = write_pcm_to_wav(retry_bytes, target_dir / retry_filename)\n",
    "                except Exception as exc:\n",
    "                    print(f\"[warning] TTS retry failed for node {node_id}: {exc}\")\n",
    "\n",
    "        for choice in choices:\n",
    "            choice_id = choice.get('id') or choice.get('choice_id')\n",
    "            choice_text = (choice.get('text') or choice.get('ChoiceText') or '').strip()\n",
    "            choice_audio_path = None\n",
    "            if choice_id and choice_text:\n",
    "                choice_filename = f\"{node_id}_{choice_id}.wav\"\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[dry-run] Skipping choice TTS for {node_id} option {choice_id}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        choice_bytes = call_gemini_tts(choice_text, model=model, voice_name=voice_name)\n",
    "                        if choice_bytes:\n",
    "                            choice_audio_path = write_pcm_to_wav(choice_bytes, target_dir / choice_filename)\n",
    "                    except Exception as exc:\n",
    "                        print(f\"[warning] TTS choice failed for {node_id} option {choice_id}: {exc}\")\n",
    "            choice_audio_entries.append(\n",
    "                {\n",
    "                    'choice_id': choice_id,\n",
    "                    'choice_text': choice_text or None,\n",
    "                    'audio': str(choice_audio_path) if choice_audio_path else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                'node_id': node_id,\n",
    "                'scene_text': scene_text or None,\n",
    "                'scene_audio': str(scene_audio_path) if scene_audio_path else None,\n",
    "                'retry_text': retry_text or None,\n",
    "                'retry_audio': str(retry_audio_path) if retry_audio_path else None,\n",
    "                'choices': choice_audio_entries if choice_audio_entries else None,\n",
    "            }\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f17cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Stage Test Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea32521",
   "metadata": {},
   "source": [
    "Define a sample user brief for ad-hoc testing. Modify values as needed before invoking individual stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b04471b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_brief = {\n",
    "    'child_age': 4,\n",
    "    'theme': 'Be carefull when crossing the road',\n",
    "    # Describe the characters you want to be in this story.\n",
    "    'character_preferences': 'My child is Asian, his name is Jacky, he is a boy',\n",
    "    'plot_preferences': 'take place in space',\n",
    "    # Other requirements for this story\n",
    "    'parental_guidance': '',\n",
    "}\n",
    "desired_choice_count = 3\n",
    "target_node_count = 10\n",
    "reference_image_limit = None\n",
    "story_image_limit = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25062479",
   "metadata": {},
   "source": [
    "### Stage 1 \u2013 Educational Theme Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1cb1f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Calling GPT-5 for stage1_learning_framework... (attempt 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stage1_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_stage1_learning_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_user_brief\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStage 1 raw output stored at:\u001b[39m\u001b[38;5;124m'\u001b[39m, (RUN_OUTPUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage1_learning_framework.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage1_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[175], line 11\u001b[0m, in \u001b[0;36mrun_stage1_learning_framework\u001b[0;34m(user_inputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m context \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild_age\u001b[39m\u001b[38;5;124m'\u001b[39m: user_inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild_age\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheme\u001b[39m\u001b[38;5;124m'\u001b[39m: user_inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheme\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_preferences\u001b[39m\u001b[38;5;124m'\u001b[39m: user_inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_preferences\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m render_prompt(template, context)\n\u001b[0;32m---> 11\u001b[0m raw_text \u001b[38;5;241m=\u001b[39m \u001b[43mcall_gpt5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstage1_learning_framework\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m parsed \u001b[38;5;241m=\u001b[39m parse_json_response(raw_text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage1_learning_framework\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m raw_text \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stage_result(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage1_learning_framework\u001b[39m\u001b[38;5;124m'\u001b[39m, prompt_text, raw_text, parsed)\n",
      "Cell \u001b[0;32mIn[173], line 75\u001b[0m, in \u001b[0;36mcall_gpt5\u001b[0;34m(prompt, stage, model, reasoning_effort, instructions, max_output_tokens, temperature, output_suffix, stream, max_retries)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\ud83d\udd04 Calling GPT-5 for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moutput_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# If we get here, the request succeeded\u001b[39;00m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/openai/resources/responses/responses.py:828\u001b[0m, in \u001b[0;36mResponses.create\u001b[0;34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/responses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_output_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_response_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http_proxy.py:343\u001b[0m, in \u001b[0;36mTunnelHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m HTTP11Connection(\n\u001b[1;32m    337\u001b[0m                 origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_origin,\n\u001b[1;32m    338\u001b[0m                 stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    339\u001b[0m                 keepalive_expiry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keepalive_expiry,\n\u001b[1;32m    340\u001b[0m             )\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stage1_result = run_stage1_learning_framework(sample_user_brief)\n",
    "print('Stage 1 raw output stored at:', (RUN_OUTPUT_DIR / 'stage1_learning_framework.json'))\n",
    "if stage1_result['parsed']:\n",
    "    display(JSON(stage1_result['parsed']))\n",
    "else:\n",
    "    print('No parsed JSON available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb36c41",
   "metadata": {},
   "source": [
    "### Stage 2 \u2013 Story Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ff560",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_result = run_stage2_story(sample_user_brief, stage1_result['parsed'])\n",
    "print('Stage 2 raw output stored at:', (RUN_OUTPUT_DIR / 'stage2_story_generator.json'))\n",
    "if stage2_result['parsed']:\n",
    "    display(JSON(stage2_result['parsed']))\n",
    "else:\n",
    "    print('No parsed JSON available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588022f",
   "metadata": {},
   "source": [
    "### Reference Image Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f46170",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_bible = stage2_result['parsed'].get('character_bible', []) if stage2_result['parsed'] else []\n",
    "reference_prompts_result = run_reference_image_prompts(character_bible, STYLE_GUIDE.get('style_prompt_suffix', ''), '')\n",
    "print('Reference prompt raw output stored at:', (RUN_OUTPUT_DIR / 'reference_image_generator.json'))\n",
    "if reference_prompts_result['parsed']:\n",
    "    display(JSON(reference_prompts_result['parsed']))\n",
    "else:\n",
    "    print('No reference prompt data available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00301d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_image_assets = generate_reference_images(\n",
    "    reference_prompts_result['parsed'],\n",
    "    model=DEFAULT_IMAGE_MODEL,\n",
    "    style_suffix=STYLE_GUIDE.get('style_prompt_suffix'),\n",
    "    negative_prompt=STYLE_GUIDE.get('negative_prompt'),\n",
    ")\n",
    "reference_image_lookup = build_reference_lookup(reference_image_assets)\n",
    "display(JSON({'reference_images': reference_image_assets}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64dddf",
   "metadata": {},
   "source": [
    "### Stage 3 \u2013 Story Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_result = run_stage3_nodes(stage2_result['parsed'], node_count=target_node_count)\n",
    "print('Stage 3 raw output stored at:', (RUN_OUTPUT_DIR / 'stage3_story_orchestrator.xml'))\n",
    "preview = stage3_result['parsed'].get('nodes', [])[:3]\n",
    "if preview:\n",
    "    display(JSON({'preview_nodes': preview}))\n",
    "else:\n",
    "    print('No node previews available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c364e8",
   "metadata": {},
   "source": [
    "### Stage 4 \u2013 Choice Node Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage4_result = run_stage4_choices(stage3_result['raw'], stage1_result['parsed'], desired_choice_count)\n",
    "print('Stage 4 raw output stored at:', (RUN_OUTPUT_DIR / 'stage4_choice_node_generator.xml'))\n",
    "preview = stage4_result['parsed'].get('choice_node_set', {}).get('question_nodes', [])\n",
    "if preview:\n",
    "    display(JSON({'question_nodes_preview': preview[:2]}))\n",
    "else:\n",
    "    print('No choice node preview available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bdd1f",
   "metadata": {},
   "source": [
    "### Stage 5 \u2013 Storyboarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e22baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_id = None\n",
    "if stage2_result['parsed']:\n",
    "    story_id = stage2_result['parsed'].get('story_metadata', {}).get('story_id')\n",
    "story_graph_payload = story_records_to_graph(stage4_result['parsed'].get('nodes', []), story_id=story_id)\n",
    "choice_node_set_payload = stage4_result['parsed'].get('choice_node_set', {})\n",
    "character_bible_payload = stage2_result['parsed'].get('character_bible', []) if stage2_result['parsed'] else []\n",
    "reference_images_payload = reference_prompts_result['parsed'] if reference_prompts_result['parsed'] else {'reference_prompts': []}\n",
    "stage5_result = run_stage5_storyboard(\n",
    "    story_graph_payload,\n",
    "    choice_node_set_payload,\n",
    "    character_bible_payload,\n",
    "    reference_images_payload,\n",
    "    STYLE_GUIDE,\n",
    "    story_node_graph_xml=stage4_result['raw'],\n",
    ")\n",
    "print('Stage 5 raw output stored at:', (RUN_OUTPUT_DIR / 'stage5_storyboarder.xml'))\n",
    "story_metadata_payload = stage2_result['parsed'].get('story_metadata', {}) if stage2_result['parsed'] else {}\n",
    "if stage5_result['raw']:\n",
    "    combined_xml, combined_path = save_story_with_metadata(story_metadata_payload, stage5_result['raw'])\n",
    "    if combined_path:\n",
    "        print('Storyboard with metadata saved at:', combined_path)\n",
    "    else:\n",
    "        print('Storyboard metadata embedding skipped.')\n",
    "else:\n",
    "    print('Storyboard XML missing; skipping metadata embedding.')\n",
    "if stage5_result['parsed']:\n",
    "    display(JSON(stage5_result['parsed']))\n",
    "else:\n",
    "    print('No storyboard data available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4dd66a",
   "metadata": {},
   "source": [
    "### Text-to-Speech \u2013 Gemini 2.5 Flash TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tts_results = []\n",
    "tts_output_dir = RUN_OUTPUT_DIR / 'audio'\n",
    "if stage5_result['parsed']:\n",
    "    storyboard_payload = stage5_result['parsed'].get('storyboard_nodes', {})\n",
    "    if storyboard_payload:\n",
    "        tts_results = generate_storyboard_tts(\n",
    "            storyboard_payload,\n",
    "            voice_name=DEFAULT_TTS_VOICE,\n",
    "        )\n",
    "        print('TTS audio saved to:', tts_output_dir)\n",
    "        if tts_results:\n",
    "            display(JSON({'tts_audio': tts_results}))\n",
    "    else:\n",
    "        print('Storyboard payload missing; skipping TTS generation.')\n",
    "else:\n",
    "    print('No storyboard data available; skipping TTS generation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32cac8",
   "metadata": {},
   "source": [
    "### Image Generation \u2013 Nano Banana / Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "storyboard_payload = stage5_result['parsed'].get('storyboard_nodes', {}) if stage5_result['parsed'] else {}\n",
    "if storyboard_payload:\n",
    "    story_image_results = generate_story_images_with_references(\n",
    "        storyboard_payload,\n",
    "        reference_image_assets,\n",
    "        model=DEFAULT_IMAGE_MODEL,\n",
    "        style_guide=STYLE_GUIDE,\n",
    "        limit=story_image_limit,\n",
    "    )\n",
    "    display(JSON({'story_images': story_image_results}))\n",
    "else:\n",
    "    print('Storyboard payload missing; cannot generate images.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e62edd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pipeline Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12d3c9",
   "metadata": {},
   "source": [
    "Execute the end-to-end prompt flow (Stages 1-5) in one call. Toggle `DRY_RUN` if you only want to render prompts without hitting the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d68a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_full_pipeline(user_brief: Dict[str, Any], *, node_count: int = 15, choice_count: int = 4) -> Dict[str, Any]:\n",
    "    stage1 = run_stage1_learning_framework(user_brief)\n",
    "    stage2 = run_stage2_story(user_brief, stage1['parsed'])\n",
    "    stage3 = run_stage3_nodes(stage2['parsed'], node_count=node_count)\n",
    "    stage4 = run_stage4_choices(stage3['raw'], stage1['parsed'], choice_count)\n",
    "    character_bible = stage2['parsed'].get('character_bible', []) if stage2['parsed'] else []\n",
    "    reference_prompts = run_reference_image_prompts(\n",
    "        character_bible,\n",
    "        STYLE_GUIDE.get('style_prompt_suffix', ''),\n",
    "        '',\n",
    "    )\n",
    "    reference_assets = generate_reference_images(\n",
    "        reference_prompts['parsed'],\n",
    "        model=DEFAULT_IMAGE_MODEL,\n",
    "        style_suffix=STYLE_GUIDE.get('style_prompt_suffix'),\n",
    "        negative_prompt=STYLE_GUIDE.get('negative_prompt'),\n",
    "        limit=reference_image_limit,\n",
    "    )\n",
    "    story_id = stage2['parsed'].get('story_metadata', {}).get('story_id') if stage2['parsed'] else None\n",
    "    story_graph_payload = story_records_to_graph(stage4['parsed'].get('nodes', []), story_id=story_id)\n",
    "    choice_node_set_payload = stage4['parsed'].get('choice_node_set', {})\n",
    "    reference_payload = reference_prompts['parsed'] if reference_prompts['parsed'] else {'reference_prompts': []}\n",
    "    storyboard = run_stage5_storyboard(\n",
    "        story_graph_payload,\n",
    "        choice_node_set_payload,\n",
    "        character_bible,\n",
    "        reference_payload,\n",
    "        STYLE_GUIDE,\n",
    "        story_node_graph_xml=stage4['raw'],\n",
    "    )\n",
    "    storyboard_payload = storyboard['parsed'].get('storyboard_nodes', {}) if storyboard['parsed'] else {}\n",
    "    tts_results = (\n",
    "        generate_storyboard_tts(\n",
    "            storyboard_payload,\n",
    "            voice_name=DEFAULT_TTS_VOICE,\n",
    "        )\n",
    "        if storyboard_payload\n",
    "        else []\n",
    "    )\n",
    "    story_metadata = stage2['parsed'].get('story_metadata', {}) if stage2['parsed'] else {}\n",
    "    combined_storyboard_xml, combined_storyboard_path = '', None\n",
    "    if storyboard['raw']:\n",
    "        combined_storyboard_xml, combined_storyboard_path = save_story_with_metadata(\n",
    "            story_metadata,\n",
    "            storyboard['raw'],\n",
    "            filename='storyboard_with_metadata_full_pipeline.xml',\n",
    "        )\n",
    "    story_images = generate_story_images_with_references(\n",
    "        storyboard_payload,\n",
    "        reference_assets,\n",
    "        model=DEFAULT_IMAGE_MODEL,\n",
    "        style_guide=STYLE_GUIDE,\n",
    "        limit=story_image_limit,\n",
    "    )\n",
    "    return {\n",
    "        'stage1': stage1,\n",
    "        'stage2': stage2,\n",
    "        'stage3': stage3,\n",
    "        'stage4': stage4,\n",
    "        'reference_prompts': reference_prompts,\n",
    "        'reference_image_assets': reference_assets,\n",
    "        'stage5': storyboard,\n",
    "        'story_images': story_images,\n",
    "        'storyboard_with_metadata': {\n",
    "            'xml': combined_storyboard_xml,\n",
    "            'path': str(combined_storyboard_path) if combined_storyboard_path else None,\n",
    "        },\n",
    "        'tts_audio': tts_results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa5719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Calling GPT-5 for stage1_learning_framework... (attempt 1)\n",
      "\ud83d\udd04 Calling GPT-5 for stage2_story_generator... (attempt 1)\n",
      "\ud83d\udd04 Calling GPT-5 for stage3_story_orchestrator... (attempt 1)\n",
      "\ud83d\udd04 Calling GPT-5 for stage4_choice_node_generator... (attempt 1)\n",
      "\ud83d\udd04 Calling GPT-5 for reference_image_generator... (attempt 1)\n",
      "\ud83d\udd04 Calling GPT-5 for stage5_storyboarder... (attempt 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_pipeline_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_user_brief\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_node_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoice_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesired_choice_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m storyboard_nodes_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_pipeline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage5\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[167], line 33\u001b[0m, in \u001b[0;36mrun_full_pipeline\u001b[0;34m(user_brief, node_count, choice_count)\u001b[0m\n\u001b[1;32m     23\u001b[0m storyboard \u001b[38;5;241m=\u001b[39m run_stage5_storyboard(\n\u001b[1;32m     24\u001b[0m     story_graph_payload,\n\u001b[1;32m     25\u001b[0m     choice_node_set_payload,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     story_node_graph_xml\u001b[38;5;241m=\u001b[39mstage4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m storyboard_payload \u001b[38;5;241m=\u001b[39m storyboard[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoryboard_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m, {}) \u001b[38;5;28;01mif\u001b[39;00m storyboard[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     32\u001b[0m tts_results \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mgenerate_storyboard_tts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstoryboard_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvoice_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_TTS_VOICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m storyboard_payload\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m story_metadata \u001b[38;5;241m=\u001b[39m stage2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstory_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m, {}) \u001b[38;5;28;01mif\u001b[39;00m stage2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     41\u001b[0m combined_storyboard_xml, combined_storyboard_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[165], line 81\u001b[0m, in \u001b[0;36mgenerate_storyboard_tts\u001b[0;34m(storyboard, model, voice_name, output_dir, limit)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         audio_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mcall_gemini_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvoice_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m audio_bytes:\n\u001b[1;32m     83\u001b[0m             scene_audio_path \u001b[38;5;241m=\u001b[39m write_pcm_to_wav(audio_bytes, target_dir \u001b[38;5;241m/\u001b[39m filename)\n",
      "Cell \u001b[0;32mIn[165], line 44\u001b[0m, in \u001b[0;36mcall_gemini_tts\u001b[0;34m(text, model, voice_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_gemini_tts\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_TTS_MODEL, voice_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_TTS_VOICE) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m     36\u001b[0m     generation_config \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentConfig(\n\u001b[1;32m     37\u001b[0m         response_modalities\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUDIO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m         speech_config\u001b[38;5;241m=\u001b[39mtypes\u001b[38;5;241m.\u001b[39mSpeechConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         ),\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgemini_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_inline_audio_bytes(response)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/google/genai/models.py:5001\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4999\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5000\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5001\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5002\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[1;32m   5003\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5005\u001b[0m   function_map \u001b[38;5;241m=\u001b[39m _extra_utils\u001b[38;5;241m.\u001b[39mget_function_map(parsed_config)\n\u001b[1;32m   5006\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/google/genai/models.py:3813\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3810\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   3811\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 3813\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3814\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   3815\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m   3818\u001b[0m     config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould_return_http_response\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3819\u001b[0m ):\n\u001b[1;32m   3820\u001b[0m   return_value \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentResponse(sdk_http_response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/google/genai/_api_client.py:1306\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1298\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1302\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SdkHttpResponse:\n\u001b[1;32m   1303\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m   1304\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m   1305\u001b[0m   )\n\u001b[0;32m-> 1306\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m   response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1308\u001b[0m       response\u001b[38;5;241m.\u001b[39mresponse_stream[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresponse_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1309\u001b[0m   )\n\u001b[1;32m   1310\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, body\u001b[38;5;241m=\u001b[39mresponse_body)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/google/genai/_api_client.py:1142\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     retry \u001b[38;5;241m=\u001b[39m tenacity\u001b[38;5;241m.\u001b[39mRetrying(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs)\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[0;32m-> 1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/tenacity/__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/tenacity/__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/tenacity/__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/tenacity/__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/google/genai/_api_client.py:1112\u001b[0m, in \u001b[0;36mBaseApiClient._request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m   1109\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m   1110\u001b[0m   )\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_httpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m   errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_response(response)\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m   1121\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m   1122\u001b[0m   )\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:928\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    927\u001b[0m     response\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:922\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 922\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_models.py:881\u001b[0m, in \u001b[0;36mResponse.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03mRead and return the response content.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_models.py:897\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    898\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_models.py:951\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    948\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_client.py:153\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpx/_transports/default.py:127\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Tomo/Jacky/talkbookv1/venv/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_pipeline_results = run_full_pipeline(sample_user_brief, node_count=target_node_count, choice_count=desired_choice_count)\n",
    "storyboard_nodes_count = 0\n",
    "if full_pipeline_results['stage5']['parsed']:\n",
    "    storyboard_nodes_count = len(full_pipeline_results['stage5']['parsed'].get('storyboard_nodes', {}).get('nodes', []))\n",
    "reference_image_count = len(full_pipeline_results.get('reference_image_assets', []))\n",
    "story_image_count = len(full_pipeline_results.get('story_images', []))\n",
    "tts_audio_count = len(full_pipeline_results.get('tts_audio', []))\n",
    "print(\n",
    "    'Pipeline finished. '\n",
    "    f'Storyboard nodes: {storyboard_nodes_count}, '\n",
    "    f'reference images: {reference_image_count}, '\n",
    "    f'story node images: {story_image_count}, '\n",
    "    f'tts clips: {tts_audio_count}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b68e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Validation & Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86d77c",
   "metadata": {},
   "source": [
    "Lightweight checks on node counts, choice distribution, and storyboard completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def summarize_choice_nodes(choice_node_set: Dict[str, Any]) -> Dict[str, int]:\n",
    "    question_nodes = choice_node_set.get('question_nodes', [])\n",
    "    outcome_nodes = choice_node_set.get('outcome_nodes', [])\n",
    "    return {\n",
    "        'question_count': len(question_nodes),\n",
    "        'outcome_count': len(outcome_nodes),\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_story_graph(nodes: List[Dict[str, Any]], expected_count: int) -> Dict[str, Any]:\n",
    "    id_counter = Counter(node.get('node_id') for node in nodes)\n",
    "    return {\n",
    "        'total_nodes': len(nodes),\n",
    "        'duplicate_ids': [node_id for node_id, count in id_counter.items() if count > 1],\n",
    "        'missing_expected_count': len(nodes) != expected_count,\n",
    "    }\n",
    "\n",
    "validation_summary = {\n",
    "    'stage3_nodes': validate_story_graph(stage3_result['parsed'].get('nodes', []), target_node_count),\n",
    "    'stage4_choices': summarize_choice_nodes(stage4_result['parsed'].get('choice_node_set', {})),\n",
    "    'storyboard_nodes': len(stage5_result['parsed'].get('storyboard_nodes', {}).get('nodes', [])) if stage5_result['parsed'] else 0,\n",
    "    'reference_images_generated': len(reference_image_assets),\n",
    "    'story_images_generated': len(story_image_results) if 'story_image_results' in globals() else 0,\n",
    "    'tts_audio_generated': len(tts_results) if 'tts_results' in globals() else 0,\n",
    "}\n",
    "\n",
    "display(JSON(validation_summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3638a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Troubleshooting & Next Steps\n",
    "- Toggle `DRY_RUN = True` to rehearse prompts without incurring API calls.\n",
    "- Adjust `desired_choice_count` and `target_node_count` to stress-test pacing variations.\n",
    "- Replace `reference_images_payload` with actual Nano Banana reference image metadata once generated.\n",
    "- Integrate Stage 6 guardrail prompts when ready and extend the pipeline runner accordingly.\n",
    "- Consider caching stage outputs to disk (beyond the timestamped folder) for regression comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}